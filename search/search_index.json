{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"EasyLLM","text":"<p>EasyLLM is an open source project that provides helpful tools and methods for working with large language models (LLMs), both open source and closed source. </p> <p>EasyLLM implements clients that are compatible with OpenAI's Completion API. This means you can easily replace <code>openai.ChatCompletion</code> with, for example, <code>huggingface.ChatCompletion</code>.</p> <ul> <li>ChatCompletion Clients</li> <li>Prompt Utils</li> <li>Examples</li> </ul>"},{"location":"#getting-started","title":"\ud83d\ude80 Getting Started","text":"<p>Install EasyLLM via pip:</p> <pre><code>pip install easyllm\n</code></pre> <p>Then import and start using the clients:</p> <p><pre><code>from easyllm.clients import huggingface\n# helper to build llama2 prompt\nhuggingface.prompt_builder = \"llama2\"\nresponse = huggingface.ChatCompletion.create(\nmodel=\"meta-llama/Llama-2-70b-chat-hf\",\nmessages=[\n{\"role\": \"system\", \"content\": \"\\nYou are a helpful assistant speaking like a pirate. argh!\"},\n{\"role\": \"user\", \"content\": \"What is the sun?\"},\n],\ntemperature=0.9,\ntop_p=0.6,\nmax_tokens=256,\n)\nprint(response)\n</code></pre> the result will look like </p> <pre><code>{\n\"id\": \"hf-lVC2iTMkFJ\",\n  \"object\": \"chat.completion\",\n  \"created\": 1690661144,\n  \"model\": \"meta-llama/Llama-2-70b-chat-hf\",\n  \"choices\": [\n{\n\"index\": 0,\n      \"message\": {\n\"role\": \"assistant\",\n        \"content\": \" Arrrr, the sun be a big ol' ball o' fire in the sky, me hearty! It be the source o' light and warmth for our fair planet, and it be a mighty powerful force, savvy? Without the sun, we'd be sailin' through the darkness, lost and cold, so let's give a hearty \\\"Yarrr!\\\" for the sun, me hearties! Arrrr!\"\n},\n      \"finish_reason\": null\n    }\n],\n  \"usage\": {\n\"prompt_tokens\": 111,\n    \"completion_tokens\": 299,\n    \"total_tokens\": 410\n}\n}\n</code></pre> <p>Check out other examples:</p> <ul> <li>Detailed ChatCompletion Example</li> <li>Example how to stream chat requests</li> <li>Example how to stream text requests</li> <li>Detailed Completion Example</li> <li>Create Embeddings</li> </ul>"},{"location":"#migration-from-openai-to-huggingface","title":"\ud83d\udcaa\ud83c\udffb Migration from OpenAI to HuggingFace","text":"<p>Migrating from OpenAI to HuggingFace is easy. Just change the import statement and the client you want to use and optionally the prompt builder.</p> <pre><code>- import openai\n+ from easyllm.clients import huggingface\n+ huggingface.prompt_builder = \"llama2\"\n- response = openai.ChatCompletion.create(\n+ response = huggingface.ChatCompletion.create(\n-    model=\"gpt-3.5-turbo\",\n+    model=\"meta-llama/Llama-2-70b-chat-hf\",\n   messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n    ],\n)\n</code></pre> <p>Make sure when you switch your client that your hyperparameters are still valid. For example, <code>temperature</code> of GPT-3 might be different than <code>temperature</code> of <code>Llama-2</code>.</p>"},{"location":"#key-features","title":"\u2611\ufe0f Key Features","text":""},{"location":"#compatible-clients","title":"\ud83e\udd1d Compatible Clients","text":"<ul> <li>Implementation of clients compatible with OpenAI API format of <code>openai.ChatCompletion</code>.</li> <li>Easily switch between different LLMs like <code>openai.ChatCompletion</code> and <code>huggingface.ChatCompletion</code> by changing one line of code. </li> <li>Support for streaming of completions, checkout example How to stream completions.</li> </ul>"},{"location":"#helper-modules","title":"\u2699\ufe0f Helper Modules \u2699\ufe0f","text":"<ul> <li> <p><code>evol_instruct</code> (work in progress) - Use evolutionary algorithms create instructions for LLMs.</p> </li> <li> <p><code>prompt_utils</code> - Helper methods to easily convert between prompt formats like OpenAI Messages to prompts for open source models like Llama 2.</p> </li> </ul>"},{"location":"#citation-acknowledgements","title":"\ud83d\udcd4 Citation &amp; Acknowledgements","text":"<p>If you use EasyLLM, please share it with me on social media or email. I would love to hear about it! You can also cite the project using the following BibTeX:</p> <pre><code>@software{Philipp_Schmid_EasyLLM_2023,\nauthor = {Philipp Schmid},\nlicense = {Apache-2.0},\nmonth = juj,\ntitle = {EasyLLM: Streamlined Tools for LLMs},\nurl = {https://github.com/philschmid/easyllm},\nyear = {2023}\n}\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#with-pip-recommended","title":"with pip recommended","text":"<p>EasyLLM is published as a [Python package] and can be installed with <code>pip</code> from pypi or from the Github repository, Open up a terminal and install.</p> LatestGithub <pre><code>pip install easyllm\n</code></pre> <pre><code>pip install git+https://github.com/philschmid/easyllm\n</code></pre>"},{"location":"prompt_utils/","title":"Prompt utilities","text":"<p>The <code>prompt_utils</code>  module contains functions to assist with converting Message's Dictionaries into prompts that can be used with <code>ChatCompletion</code> clients. </p> <p>Supported prompt formats:</p> <ul> <li>Llama 2</li> <li>Vicuna</li> <li>Hugging Face ChatML</li> <li>WizardLM</li> <li>StableBeluga2</li> <li>Open Assistant</li> </ul> <p>Prompt utils are also exporting a mapping dictionary <code>PROMPT_MAPPING</code> that maps a model name to a prompt builder function. This can be used to select the correct prompt builder function via an environment variable. </p> <pre><code>PROMPT_MAPPING = {\n\"chatml_falcon\": build_chatml_falcon_prompt,\n\"chatml_starchat\": build_chatml_starchat_prompt,\n\"llama2\": build_llama2_prompt,\n\"open_assistant\": build_open_assistant_prompt,\n\"stablebeluga\": build_stablebeluga_prompt,\n\"vicuna\": build_vicuna_prompt,\n\"wizardlm\": build_wizardlm_prompt,\n}\n</code></pre>"},{"location":"prompt_utils/#set-prompt-builder-for-client","title":"Set prompt builder for client","text":"<pre><code>from easyllm.clients import huggingface\nhuggingface.prompt_builder = \"llama2\" # vicuna, chatml_falcon, chatml_starchat, wizardlm, stablebeluga, open_assistant\n</code></pre>"},{"location":"prompt_utils/#llama-2-chat-builder","title":"Llama 2 Chat builder","text":"<p>Creates LLama 2 chat prompt for chat conversations. Learn more in the Hugging Face Blog on how to prompt Llama 2. If a <code>Message</code> with an unsupported <code>role</code> is passed, an error will be thrown.</p> <p>Example Models: </p> <ul> <li>meta-llama/Llama-2-70b-chat-hf</li> </ul> <pre><code>from easyllm.prompt_utils import build_llama2_prompt\nmessages=[\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n]\nprompt = build_llama2_prompt(messages)\n</code></pre>"},{"location":"prompt_utils/#vicuna-chat-builder","title":"Vicuna Chat builder","text":"<p>Creats a Vicuna prompt for a chat conversation. If a <code>Message</code> with an unsupported <code>role</code> is passed, an error will be thrown. Reference</p> <p>Example Models: </p> <ul> <li>ehartford/WizardLM-13B-V1.0-Uncensored</li> </ul> <pre><code>from easyllm.prompt_utils import build_vicuna_prompt\nmessages=[\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n]\nprompt = build_vicuna_prompt(messages)\n</code></pre>"},{"location":"prompt_utils/#hugging-face-chatml-builder","title":"Hugging Face ChatML builder","text":"<p>Creates a Hugging Face ChatML prompt for a chat conversation. The Hugging Face ChatML has different prompts for different Example Models, e.g. StarChat or Falcon. If a <code>Message</code> with an unsupported <code>role</code> is passed, an error will be thrown. Reference</p> <p>Example Models:  * HuggingFaceH4/starchat-beta</p>"},{"location":"prompt_utils/#starchat","title":"StarChat","text":"<pre><code>from easyllm.prompt_utils import build_chatml_starchat_prompt\nmessages=[\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n]\nprompt = build_chatml_starchat_prompt(messages)\n</code></pre>"},{"location":"prompt_utils/#falcon","title":"Falcon","text":"<pre><code>from easyllm.prompt_utils import build_chatml_falcon_prompt\nmessages=[\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n]\nprompt = build_chatml_falcon_prompt(messages)\n</code></pre>"},{"location":"prompt_utils/#wizardlm-chat-builder","title":"WizardLM Chat builder","text":"<p>Creates a WizardLM prompt for a chat conversation. If a <code>Message</code> with an unsupported <code>role</code> is passed, an error will be thrown. Reference</p> <p>Example Models:</p> <ul> <li>WizardLM/WizardLM-13B-V1.2</li> </ul> <pre><code>from easyllm.prompt_utils import build_wizardlm_prompt\nmessages=[\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n]\nprompt = build_wizardlm_prompt(messages)\n</code></pre>"},{"location":"prompt_utils/#stablebeluga2-chat-builder","title":"StableBeluga2 Chat builder","text":"<p>Creates StableBeluga2 prompt for a chat conversation. If a <code>Message</code> with an unsupported <code>role</code> is passed, an error will be thrown. Reference</p> <pre><code>from easyllm.prompt_utils import build_stablebeluga_prompt\nmessages=[\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n]\nprompt = build_stablebeluga_prompt(messages)\n</code></pre>"},{"location":"prompt_utils/#open-assistant-chat-builder","title":"Open Assistant Chat builder","text":"<p>Creates Open Assistant ChatML template. Uses <code>&lt;|prompter|&gt;</code>, <code>&lt;/s&gt;</code>, <code>&lt;|system|&gt;</code>, and <code>&lt;|assistant&gt;</code> tokens. If a . If a <code>Message</code> with an unsupported <code>role</code> is passed, an error will be thrown. Reference</p> <p>Example Models:</p> <ul> <li>OpenAssistant/llama2-13b-orca-8k-3319</li> </ul> <pre><code>from easyllm.prompt_utils import build_open_assistant_prompt\nmessages=[\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n]\nprompt = build_open_assistant_prompt(messages)\n</code></pre>"},{"location":"clients/","title":"Clients","text":"<p>In the context of EasyLLM, a \"client\" refers to the code that interfaces with a particular LLM API, e.g. OpenAI.</p> <p>Currently supported clients are:  </p> <ul> <li><code>ChatCompletion</code> - ChatCompletion clients are used to interface with LLMs that are compatible with the OpenAI ChatCompletion API.</li> <li><code>Completion</code> - Completion clients are used to interface with LLMs that are compatible with the OpenAI Completion API.</li> <li><code>Embedding</code> - Embedding clients are used to interface with LLMs that are compatible with the OpenAI Embedding API.</li> </ul> <p>Currently supported clients are:  </p>"},{"location":"clients/#hugging-face","title":"Hugging Face","text":"<ul> <li>huggingface.ChatCompletion - a client for interfacing with HuggingFace models that are compatible with the OpenAI ChatCompletion API.</li> <li>huggingface.Completion - a client for interfacing with HuggingFace models that are compatible with the OpenAI Completion API.</li> <li>huggingface.Embedding - a client for interfacing with HuggingFace models that are compatible with the OpenAI Embedding API.</li> </ul>"},{"location":"clients/huggingface/","title":"Hugging Face","text":"<p>EasyLLM provides a client for interfacing with HuggingFace models. The client is compatible with the HuggingFace Inference API, Hugging Face Inference Endpoints or any Web Service running Text Generation Inference or compatible API endpoints. </p> <ul> <li><code>huggingface.ChatCompletion</code> - a client for interfacing with HuggingFace models that are compatible with the OpenAI ChatCompletion API.</li> <li><code>huggingface.Completion</code> - a client for interfacing with HuggingFace models that are compatible with the OpenAI Completion API.</li> <li><code>huggingface.Embedding</code> - a client for interfacing with HuggingFace models that are compatible with the OpenAI Embedding API.</li> </ul>"},{"location":"clients/huggingface/#huggingfacechatcompletion","title":"<code>huggingface.ChatCompletion</code>","text":"<p>The <code>huggingface.ChatCompletion</code> client is used to interface with HuggingFace models running on Text Generation inference that are compatible with the OpenAI ChatCompletion API. Checkout the Examples for more details and How to stream completions for an example how to stream requests.</p> <pre><code>from easyllm.clients import huggingface\n# The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file.\n# huggingface.api_key=\"hf_xxx\"\nhubbingface.prompt_builder = \"llama2\"\nresponse = huggingface.ChatCompletion.create(\nmodel=\"meta-llama/Llama-2-70b-chat-hf\",\nmessages=[\n{\"role\": \"system\", \"content\": \"\\nYou are a helpful, respectful and honest assistant.\"},\n{\"role\": \"user\", \"content\": \"Knock knock.\"},\n],\ntemperature=0.9,\ntop_p=0.6,\nmax_tokens=1024,\n)\n</code></pre> <p>Supported parameters are:</p> <ul> <li><code>model</code> - The model to use for the completion. If not provided, defaults to the base url.</li> <li><code>messages</code> - <code>List[ChatMessage]</code> to use for the completion.</li> <li><code>temperature</code> - The temperature to use for the completion. Defaults to 0.9.</li> <li><code>top_p</code> - The top_p to use for the completion. Defaults to 0.6.</li> <li><code>top_k</code> - The top_k to use for the completion. Defaults to 10.</li> <li><code>n</code> - The number of completions to generate. Defaults to 1.</li> <li><code>max_tokens</code> - The maximum number of tokens to generate. Defaults to 1024.</li> <li><code>stop</code> - The stop sequence(s) to use for the completion. Defaults to None.</li> <li><code>stream</code> - Whether to stream the completion. Defaults to False.</li> <li><code>frequency_penalty</code> - The frequency penalty to use for the completion. Defaults to 1.0.</li> <li><code>debug</code> - Whether to enable debug logging. Defaults to False.</li> </ul>"},{"location":"clients/huggingface/#huggingfacecompletion","title":"<code>huggingface.Completion</code>","text":"<p>The <code>huggingface.Completion</code> client is used to interface with HuggingFace models running on Text Generation inference that are compatible with the OpenAI Completion API. Checkout the Examples for more details and How to stream completions for an example how to stream requests.</p> <pre><code>from easyllm.clients import huggingface\n# The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file.\n# huggingface.api_key=\"hf_xxx\"\nhubbingface.prompt_builder = \"llama2\"\nresponse = huggingface.Completion.create(\nmodel=\"meta-llama/Llama-2-70b-chat-hf\",\nprompt=\"What is the meaning of life?\",\ntemperature=0.9,\ntop_p=0.6,\nmax_tokens=1024,\n)\n</code></pre> <p>Supported parameters are:</p> <ul> <li><code>model</code> - The model to use for the completion. If not provided, defaults to the base url.</li> <li><code>prompt</code> -  Text to use for the completion, if prompt_builder is set, prompt will be formatted with the prompt_builder.</li> <li><code>temperature</code> - The temperature to use for the completion. Defaults to 0.9.</li> <li><code>top_p</code> - The top_p to use for the completion. Defaults to 0.6.</li> <li><code>top_k</code> - The top_k to use for the completion. Defaults to 10.</li> <li><code>n</code> - The number of completions to generate. Defaults to 1.</li> <li><code>max_tokens</code> - The maximum number of tokens to generate. Defaults to 1024.</li> <li><code>stop</code> - The stop sequence(s) to use for the completion. Defaults to None.</li> <li><code>stream</code> - Whether to stream the completion. Defaults to False.</li> <li><code>frequency_penalty</code> - The frequency penalty to use for the completion. Defaults to 1.0.</li> <li><code>debug</code> - Whether to enable debug logging. Defaults to False.</li> <li><code>echo</code> - Whether to echo the prompt. Defaults to False.</li> <li><code>logprobs</code> - Weather to return logprobs. Defaults to None.</li> </ul>"},{"location":"clients/huggingface/#huggingfaceembedding","title":"<code>huggingface.Embedding</code>","text":"<p>The <code>huggingface.Embedding</code> client is used to interface with HuggingFace models running as an API that are compatible with the OpenAI Embedding API. Checkout the Examples for more details.</p> <pre><code>from easyllm.clients import huggingface\n# The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file.\n# huggingface.api_key=\"hf_xxx\"\nembedding = huggingface.Embedding.create(\nmodel=\"sentence-transformers/all-MiniLM-L6-v2\",\ntext=\"What is the meaning of life?\",\n)\nlen(embedding[\"data\"][0][\"embedding\"])\n</code></pre> <p>Supported parameters are:</p> <ul> <li><code>model</code> - The model to use to create the embedding. If not provided, defaults to the base url.</li> <li><code>input</code> -  <code>Union[str, List[str]]</code> document(s) to embed.</li> </ul>"},{"location":"clients/huggingface/#environment-configuration","title":"Environment Configuration","text":"<p>You can configure the <code>huggingface</code> client by setting environment variables or overwriting the default values. See below on how to adjust the HF token, url and prompt builder.</p>"},{"location":"clients/huggingface/#setting-hf-token","title":"Setting HF token","text":"<p>By default the <code>huggingface</code> client will try to read the <code>HUGGINGFACE_TOKEN</code> environment variable. If this is not set, it will try to read the token from the <code>~/.huggingface</code> folder. If this is not set, it will not use a token.</p> <p>Alternatively you can set the token manually by setting <code>huggingface.api_key</code>.</p> <p>manually setting the api key:</p> <pre><code>from easyllm.clients import huggingface\nhuggingface.api_key=\"hf_xxx\"\nres = huggingface.ChatCompletion.create(...)\n</code></pre> <p>Using environment variable:</p> <pre><code># can happen elsehwere\nimport os\nos.environ[\"HUGGINGFACE_TOKEN\"] = \"hf_xxx\"\nfrom easyllm.clients import huggingface\n</code></pre>"},{"location":"clients/huggingface/#changing-url","title":"Changing url","text":"<p>By default the <code>huggingface</code> client will try to read the <code>HUGGINGFACE_API_BASE</code> environment variable. If this is not set, it will use the default url <code>https://api-inference.huggingface.co/models</code>. This is helpful if you want to use a different url like <code>https://zj5lt7pmzqzbp0d1.us-east-1.aws.endpoints.huggingface.cloud</code> or a local url like <code>http://localhost:8000</code> or an Hugging Face Inference Endpoint.</p> <p>Alternatively you can set the url manually by setting <code>huggingface.api_base</code>. If you set a custom you have to leave the <code>model</code> parameter empty. </p> <p>manually setting the api base:</p> <pre><code>from easyllm.clients import huggingface\nhuggingface.api_base=\"https://my-url\"\nres = huggingface.ChatCompletion.create(...)\n</code></pre> <p>Using environment variable:</p> <pre><code># can happen elsehwere\nimport os\nos.environ[\"HUGGINGFACE_API_BASE\"] = \"https://my-url\"\nfrom easyllm.clients import huggingface\n</code></pre>"},{"location":"clients/huggingface/#build-prompt","title":"Build Prompt","text":"<p>By default the <code>huggingface</code> client will try to read the <code>HUGGINGFACE_PROMPT</code> environment variable and tries to map the value to the <code>PROMPT_MAPPING</code> dictionary. If this is not set, it will use the default prompt builder.  You can also set it manually.</p> <p>Checkout the Prompt Utils for more details.</p> <p>manually setting the prompt builder:</p> <pre><code>from easyllm.clients import huggingface\nhuggingface.prompt_builder = \"llama2\"\nres = huggingface.ChatCompletion.create(...)\n</code></pre> <p>Using environment variable:</p> <pre><code># can happen elsehwere\nimport os\nos.environ[\"HUGGINGFACE_PROMPT\"] = \"llama2\"\nfrom easyllm.clients import huggingface\n</code></pre>"},{"location":"examples/","title":"Examples","text":"<p>Here are some examples to help you get started with the easyllm library:</p>"},{"location":"examples/#hugging-face","title":"Hugging Face","text":"Example Description Detailed ChatCompletion Example Shows how to use the ChatCompletion API to have a conversational chat with the model. Example how to stream chat requests Demonstrates streaming multiple chat requests to efficiently chat with the model. Example how to stream text requests Shows how to stream multiple text completion requests. Detailed Completion Example Uses the TextCompletion API to generate text with the model. Create Embeddings Embeds text into vector representations using the model. Hugging Face Inference Endpoints Example Example on how to use custom endpoints, e.g. Inference Endpoints or localhost. Retrieval Augmented Generation using Llama 2 Example on how to use Llama 2 70B for in-context retrival augmentation Llama 2 70B Agent/Tool use example  Example on how to use Llama 2 70B to interace with tools and could be used as an agent <p>The examples cover the main functionality of the library - chat, text completion, and embeddings. Let me know if you would like me to modify or expand the index page in any way.</p>"},{"location":"examples/chat-completion-api/","title":"How to use Chat Completion clients","text":"In\u00a0[\u00a0]: Copied! <pre># if needed, install and/or upgrade to the latest version of the EasyLLM Python library\n%pip install --upgrade easyllm\n</pre> # if needed, install and/or upgrade to the latest version of the EasyLLM Python library %pip install --upgrade easyllm  In\u00a0[6]: Copied! <pre># import the EasyLLM Python library for calling the EasyLLM API\nimport easyllm\n</pre> # import the EasyLLM Python library for calling the EasyLLM API import easyllm In\u00a0[2]: Copied! <pre>import os \n# set env for prompt builder\nos.environ[\"HUGGINGFACE_PROMPT\"] = \"llama2\" # vicuna, wizardlm, stablebeluga, open_assistant\n# os.environ[\"HUGGINGFACE_TOKEN\"] = \"hf_xxx\" \n\nfrom easyllm.clients import huggingface\n\n# Changing configuration without using environment variables\n# huggingface.api_key=\"hf_xxx\"\n# huggingface.prompt_builder = \"llama2\"\n\n\nMODEL=\"meta-llama/Llama-2-70b-chat-hf\"\n\nresponse = huggingface.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"},\n        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n        {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n        {\"role\": \"user\", \"content\": \"Cat.\"},\n    ],\n      temperature=0.9,\n      top_p=0.6,\n      max_tokens=1024,\n)\nresponse\n</pre> import os  # set env for prompt builder os.environ[\"HUGGINGFACE_PROMPT\"] = \"llama2\" # vicuna, wizardlm, stablebeluga, open_assistant # os.environ[\"HUGGINGFACE_TOKEN\"] = \"hf_xxx\"   from easyllm.clients import huggingface  # Changing configuration without using environment variables # huggingface.api_key=\"hf_xxx\" # huggingface.prompt_builder = \"llama2\"   MODEL=\"meta-llama/Llama-2-70b-chat-hf\"  response = huggingface.ChatCompletion.create(     model=MODEL,     messages=[         {\"role\": \"system\", \"content\": \"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"},         {\"role\": \"user\", \"content\": \"Knock knock.\"},         {\"role\": \"assistant\", \"content\": \"Who's there?\"},         {\"role\": \"user\", \"content\": \"Cat.\"},     ],       temperature=0.9,       top_p=0.6,       max_tokens=1024, ) response Out[2]: <pre>{'id': 'hf-5sljhaJk2y',\n 'object': 'chat.completion',\n 'created': 1691129787,\n 'model': 'meta-llama/Llama-2-70b-chat-hf',\n 'choices': [{'index': 0,\n   'message': {'role': 'assistant', 'content': ' Cat who?'},\n   'finish_reason': 'eos_token'}],\n 'usage': {'prompt_tokens': 149, 'completion_tokens': 5, 'total_tokens': 154}}</pre> <p>As you can see, the response object has a few fields:</p> <ul> <li><code>id</code>: the ID of the request</li> <li><code>object</code>: the type of object returned (e.g., <code>chat.completion</code>)</li> <li><code>created</code>: the timestamp of the request</li> <li><code>model</code>: the full name of the model used to generate the response</li> <li><code>usage</code>: the number of tokens used to generate the replies, counting prompt, completion, and total</li> <li><code>choices</code>: a list of completion objects (only one, unless you set <code>n</code> greater than 1)<ul> <li><code>message</code>: the message object generated by the model, with <code>role</code> and <code>content</code></li> <li><code>finish_reason</code>: the reason the model stopped generating text (either <code>stop</code>, or <code>length</code> if <code>max_tokens</code> limit was reached)</li> <li><code>index</code>: the index of the completion in the list of choices</li> </ul> </li> </ul> <p>Extract just the reply with:</p> In\u00a0[3]: Copied! <pre>print(response['choices'][0]['message']['content'])\n</pre> print(response['choices'][0]['message']['content']) <pre> Cat who?\n</pre> <p>Even non-conversation-based tasks can fit into the chat format, by placing the instruction in the first user message.</p> <p>For example, to ask the model to explain asynchronous programming in the style of the pirate Blackbeard, we can structure conversation as follows:</p> In\u00a0[4]: Copied! <pre># example with a system message\nresponse = huggingface.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of math teacher.\"},\n    ],\n)\n\nprint(response['choices'][0]['message']['content'])\n</pre> # example with a system message response = huggingface.ChatCompletion.create(     model=MODEL,     messages=[         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},         {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of math teacher.\"},     ], )  print(response['choices'][0]['message']['content'])  <pre> Hello, my dear students! Today, we're going to learn about a fascinating topic that will help us understand how to make our programs more efficient and responsive: asynchronous programming.\n\nImagine you're working on a project with a group of people, and you need to finish your part before others can start theirs. But, you're waiting for someone else to finish their part so you can start yours. This is similar to how asynchronous programming works.\n\nIn asynchronous programming, we break down a program into smaller parts called \"tasks.\" These tasks can run independently, without blocking other tasks from running. This means that if one task is waiting for something to happen, like a response from a server or a user input, other tasks can keep running in the meantime.\n\nLet's use a simple example to illustrate this. Imagine you're making a sandwich. You need to put the bread slices together, add the filling, and then put the sandwich in the fridge to chill. But, you can't start making the sandwich until the bread is toasted, and you can't put the sandwich in the fridge until it's assembled.\n\nIn this scenario, toasting the bread and assembling the sandwich are two separate tasks. If we were to do them synchronously, we would do them one after the other, like this:\n\n1. Toast the bread\n2. Assemble the sandwich\n3. Put the sandwich in the fridge\n\nBut, with asynchronous programming, we can do them simultaneously, like this:\n\n1. Toast the bread (starts)\n2. Assemble the sandwich (starts)\n3. Toast the bread (finishes)\n4. Put the sandwich in the fridge\n\nBy doing tasks simultaneously, we can save time and make our program more efficient. But, we need to be careful not to get confused about the order in which things happen. That's why we use special tools, like \"promises\" and \"callbacks,\" to keep track of everything.\n\nSo, my dear students, I hope this helps you understand asynchronous programming a bit better. Remember, it's all about breaking down a program into smaller, independent tasks that can run simultaneously, making our programs more efficient and responsive. Now, go forth and create some amazing programs!\n</pre> In\u00a0[5]: Copied! <pre># example without a system message and debug flag on:\nresponse = huggingface.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n    ],\n    debug=True,\n)\n\nprint(response['choices'][0]['message']['content'])\n</pre> # example without a system message and debug flag on: response = huggingface.ChatCompletion.create(     model=MODEL,     messages=[         {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},     ],     debug=True, )  print(response['choices'][0]['message']['content'])  <pre>08/04/2023 08:16:57 - DEBUG - easyllm.utils - Prompt sent to model will be:\n&lt;s&gt;[INST] Explain asynchronous programming in the style of the pirate Blackbeard. [/INST]\n08/04/2023 08:16:57 - DEBUG - easyllm.utils - Url:\nhttps://api-inference.huggingface.co/models/meta-llama/Llama-2-70b-chat-hf\n08/04/2023 08:16:57 - DEBUG - easyllm.utils - Stop sequences:\n[]\n08/04/2023 08:16:57 - DEBUG - easyllm.utils - Generation parameters:\n{'do_sample': True, 'return_full_text': False, 'max_new_tokens': 1024, 'top_p': 0.6, 'temperature': 0.9, 'stop_sequences': [], 'repetition_penalty': 1.0, 'top_k': 10, 'seed': 42}\n08/04/2023 08:16:57 - DEBUG - easyllm.utils - Response at index 0:\nindex=0 message=ChatMessage(role='assistant', content=' Ahoy matey! Yer lookin\\' fer a tale of asynchronous programming, eh? Well, settle yerself down with a pint o\\' grog and listen close, for Blackbeard\\'s got a story fer ye.\\n\\nAsynchronous programming, me hearties, be like sailin\\' a ship through treacherous waters. Ye gotta keep yer wits about ye, and watch out fer the hidden dangers that lie beneath the surface.\\n\\nImagine ye\\'re sailin\\' along, and suddenly, out o\\' the blue, a great storm brews up. The winds howl, the waves crash, and yer ship takes on water. Now, ye gotta act fast, or ye\\'ll be sent to Davy Jones\\' locker!\\n\\nBut, me hearties, ye can\\'t just abandon ship. Ye gotta batten down the hatches, and ride out the storm. And that\\'s where asynchronous programming comes in.\\n\\nAsynchronous programming be like haulin\\' up the sails, and lettin\\' the wind do the work fer ye. Ye don\\'t have to worry about the details o\\' how the wind\\'s blowin\\', or the waves crashin\\', ye just gotta keep yer ship pointed in the right direction, and let nature take its course.\\n\\nNow, I know what ye\\'re thinkin\\', \"Blackbeard, how do I know when me ship\\'s gonna make it through the storm?\" And that, me hearties, be the beauty o\\' asynchronous programming. Ye don\\'t have to know! Ye just have to trust that the winds o\\' change will carry ye through, and ye\\'ll make it to the other side, all in one piece.\\n\\nBut, me hearties, don\\'t ye be thinkin\\' this be easy. Asynchronous programming be like navigatin\\' through treacherous waters, with a crew o\\' mutinous code, and a hull full o\\' bugs. Ye gotta be prepared fer the unexpected, and have a stout heart, or ye\\'ll be walkin\\' the plank!\\n\\nSo, me hearties, there ye have it. Asynchronous programming in the style o\\' Blackbeard. May the winds o\\' change blow in yer favor, and may yer code always be free o\\' bugs! Arrr!') finish_reason='eos_token'\n Ahoy matey! Yer lookin' fer a tale of asynchronous programming, eh? Well, settle yerself down with a pint o' grog and listen close, for Blackbeard's got a story fer ye.\n\nAsynchronous programming, me hearties, be like sailin' a ship through treacherous waters. Ye gotta keep yer wits about ye, and watch out fer the hidden dangers that lie beneath the surface.\n\nImagine ye're sailin' along, and suddenly, out o' the blue, a great storm brews up. The winds howl, the waves crash, and yer ship takes on water. Now, ye gotta act fast, or ye'll be sent to Davy Jones' locker!\n\nBut, me hearties, ye can't just abandon ship. Ye gotta batten down the hatches, and ride out the storm. And that's where asynchronous programming comes in.\n\nAsynchronous programming be like haulin' up the sails, and lettin' the wind do the work fer ye. Ye don't have to worry about the details o' how the wind's blowin', or the waves crashin', ye just gotta keep yer ship pointed in the right direction, and let nature take its course.\n\nNow, I know what ye're thinkin', \"Blackbeard, how do I know when me ship's gonna make it through the storm?\" And that, me hearties, be the beauty o' asynchronous programming. Ye don't have to know! Ye just have to trust that the winds o' change will carry ye through, and ye'll make it to the other side, all in one piece.\n\nBut, me hearties, don't ye be thinkin' this be easy. Asynchronous programming be like navigatin' through treacherous waters, with a crew o' mutinous code, and a hull full o' bugs. Ye gotta be prepared fer the unexpected, and have a stout heart, or ye'll be walkin' the plank!\n\nSo, me hearties, there ye have it. Asynchronous programming in the style o' Blackbeard. May the winds o' change blow in yer favor, and may yer code always be free o' bugs! Arrr!\n</pre> In\u00a0[6]: Copied! <pre># An example of a faked few-shot conversation to prime the model into translating business jargon to simpler speech\nresponse = huggingface.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant.\"},\n        {\"role\": \"user\", \"content\": \"Help me translate the following corporate jargon into plain English.\"},\n        {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},\n        {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},\n        {\"role\": \"assistant\", \"content\": \"Things working well together will increase revenue.\"},\n        {\"role\": \"user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n        {\"role\": \"assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n    ],\n)\n\nprint(response[\"choices\"][0][\"message\"][\"content\"])\n</pre> # An example of a faked few-shot conversation to prime the model into translating business jargon to simpler speech response = huggingface.ChatCompletion.create(     model=MODEL,     messages=[         {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant.\"},         {\"role\": \"user\", \"content\": \"Help me translate the following corporate jargon into plain English.\"},         {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},         {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},         {\"role\": \"assistant\", \"content\": \"Things working well together will increase revenue.\"},         {\"role\": \"user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},         {\"role\": \"assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},         {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},     ], )  print(response[\"choices\"][0][\"message\"][\"content\"])  <pre>08/04/2023 08:16:57 - DEBUG - easyllm.utils - Prompt sent to model will be:\n&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\nYou are a helpful, pattern-following assistant.\n&lt;&lt;/SYS&gt;&gt;\n\nHelp me translate the following corporate jargon into plain English. [/INST] Sure, I'd be happy to!&lt;/s&gt;&lt;s&gt;[INST] New synergies will help drive top-line growth. [/INST] Things working well together will increase revenue.&lt;/s&gt;&lt;s&gt;[INST] Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage. [/INST] Let's talk later when we're less busy about how to do better.&lt;/s&gt;&lt;s&gt;[INST] This late pivot means we don't have time to boil the ocean for the client deliverable. [/INST]\n08/04/2023 08:16:57 - DEBUG - easyllm.utils - Url:\nhttps://api-inference.huggingface.co/models/meta-llama/Llama-2-70b-chat-hf\n08/04/2023 08:16:57 - DEBUG - easyllm.utils - Stop sequences:\n[]\n08/04/2023 08:16:57 - DEBUG - easyllm.utils - Generation parameters:\n{'do_sample': True, 'return_full_text': False, 'max_new_tokens': 1024, 'top_p': 0.6, 'temperature': 0.9, 'stop_sequences': [], 'repetition_penalty': 1.0, 'top_k': 10, 'seed': 42}\n08/04/2023 08:16:57 - DEBUG - easyllm.utils - Response at index 0:\nindex=0 message=ChatMessage(role='assistant', content=\" We've changed direction too late to do a complete job for the client.\") finish_reason='eos_token'\n We've changed direction too late to do a complete job for the client.\n</pre> <p>Not every attempt at engineering conversations will succeed at first.</p> <p>If your first attempts fail, don't be afraid to experiment with different ways of priming or conditioning the model.</p> <p>As an example, one developer discovered an increase in accuracy when they inserted a user message that said \"Great job so far, these have been perfect\" to help condition the model into providing higher quality responses.</p> <p>For more ideas on how to lift the reliability of the models, consider reading our guide on techniques to increase reliability. It was written for non-chat models, but many of its principles still apply.</p>"},{"location":"examples/chat-completion-api/#how-to-use-chat-completion-clients","title":"How to use Chat Completion clients\u00b6","text":"<p>EasyLLM can be used as an abstract layer to replace <code>gpt-3.5-turbo</code> and <code>gpt-4</code> with open source models.</p> <p>You can change your own applications from the OpenAI API, by simply changing the client.</p> <p>Chat models take a series of messages as input, and return an AI-written message as output.</p> <p>This guide illustrates the chat format with a few example API calls.</p>"},{"location":"examples/chat-completion-api/#1-import-the-easyllm-library","title":"1. Import the easyllm library\u00b6","text":""},{"location":"examples/chat-completion-api/#2-an-example-chat-api-call","title":"2. An example chat API call\u00b6","text":"<p>A chat API call has two required inputs:</p> <ul> <li><code>model</code>: the name of the model you want to use (e.g., <code>meta-llama/Llama-2-70b-chat-hf</code>) or leave it empty to just call the api</li> <li><code>messages</code>: a list of message objects, where each object has two required fields:<ul> <li><code>role</code>: the role of the messenger (either <code>system</code>, <code>user</code>, or <code>assistant</code>)</li> <li><code>content</code>: the content of the message (e.g., <code>Write me a beautiful poem</code>)</li> </ul> </li> </ul> <p>Compared to OpenAI api is the <code>huggingface</code> module also exposing a <code>prompt_builder</code> and <code>stop_sequences</code> parameter you can use to customize the prompt and stop sequences. The EasyLLM package comes with prompt builder utilities.</p> <p>Let's look at an example chat API calls to see how the chat format works in practice.</p>"},{"location":"examples/chat-completion-api/#3-few-shot-prompting","title":"3. Few-shot prompting\u00b6","text":"<p>In some cases, it's easier to show the model what you want rather than tell the model what you want.</p> <p>One way to show the model what you want is with faked example messages.</p> <p>For example:</p>"},{"location":"examples/get-embeddings/","title":"Get embeddings","text":"In\u00a0[\u00a0]: Copied! <pre># if needed, install and/or upgrade to the latest version of the OpenAI Python library\n%pip install --upgrade easyllm\n</pre> # if needed, install and/or upgrade to the latest version of the OpenAI Python library %pip install --upgrade easyllm  In\u00a0[\u00a0]: Copied! <pre># import the EasyLLM Python library for calling the EasyLLM API\nimport easyllm\n</pre> # import the EasyLLM Python library for calling the EasyLLM API import easyllm In\u00a0[2]: Copied! <pre># import os \n# os.environ[\"HUGGINGFACE_TOKEN\"] = \"hf_xxx\"  # Use Environment Variable\n\nfrom easyllm.clients import huggingface\n\n# The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file.\n# huggingface.api_key=\"hf_xxx\"\n\nembedding = huggingface.Embedding.create(\n    model=\"sentence-transformers/all-MiniLM-L6-v2\",\n    input=\"That's a nice car.\",\n)\n\nlen(embedding[\"data\"][0][\"embedding\"])\n</pre> # import os  # os.environ[\"HUGGINGFACE_TOKEN\"] = \"hf_xxx\"  # Use Environment Variable  from easyllm.clients import huggingface  # The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file. # huggingface.api_key=\"hf_xxx\"  embedding = huggingface.Embedding.create(     model=\"sentence-transformers/all-MiniLM-L6-v2\",     input=\"That's a nice car.\", )  len(embedding[\"data\"][0][\"embedding\"]) Out[2]: <pre>384</pre> <p>Batched Request</p> In\u00a0[3]: Copied! <pre>from easyllm.clients import huggingface\n\n# The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file.\n# huggingface.api_key=\"hf_xxx\"\n\nembedding = huggingface.Embedding.create(\n    model=\"sentence-transformers/all-MiniLM-L6-v2\",\n    input=[\"What is the meaning of life?\",\"test\"],\n)\n\nlen(embedding[\"data\"])\n</pre> from easyllm.clients import huggingface  # The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file. # huggingface.api_key=\"hf_xxx\"  embedding = huggingface.Embedding.create(     model=\"sentence-transformers/all-MiniLM-L6-v2\",     input=[\"What is the meaning of life?\",\"test\"], )  len(embedding[\"data\"]) Out[3]: <pre>2</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/get-embeddings/#how-to-create-embeddings","title":"How to create embeddings\u00b6","text":"<p>In this notebook, we will show you how to create embeddings for your own text data and and open source model from Hugging Face hosted as an endpoint on Hugging Face Inference API.</p>"},{"location":"examples/get-embeddings/#1-import-the-easyllm-library","title":"1. Import the easyllm library\u00b6","text":""},{"location":"examples/get-embeddings/#2-an-example-chat-api-call","title":"2. An example chat API call\u00b6","text":"<p>A embedding API call has two required inputs:</p> <ul> <li><code>model</code>: the name of the model you want to use (e.g., <code>sentence-transformers/all-MiniLM-L6-v2</code>) or leave it empty to just call the api</li> <li><code>input</code>: a string or list of strings you want to embed</li> </ul> <p>Let's look at an example API calls to see how the chat format works in practice.</p>"},{"location":"examples/inference-endpoints-example/","title":"Hugging Face Inference Endpoints Example","text":"In\u00a0[\u00a0]: Copied! <pre># if needed, install and/or upgrade to the latest version of the OpenAI Python library\n%pip install --upgrade easyllm\n</pre> # if needed, install and/or upgrade to the latest version of the OpenAI Python library %pip install --upgrade easyllm  In\u00a0[1]: Copied! <pre>from easyllm.clients import huggingface\n\n# Here we overwrite the defaults, you can also use environment variables\nhuggingface.prompt_builder = \"llama2\"\nhuggingface.api_base = \"YOUR_ENDPOINT_URL\"\n\n# The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file.\n# huggingface.api_key=\"hf_xxx\"\n\nresponse = huggingface.ChatCompletion.create(\n    messages=[\n        {\"role\": \"system\", \"content\": \"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"},\n        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n        {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n        {\"role\": \"user\", \"content\": \"Apple.\"},\n    ],\n      temperature=0.9,\n      top_p=0.6,\n      max_tokens=1024,\n)\nresponse\n</pre> from easyllm.clients import huggingface  # Here we overwrite the defaults, you can also use environment variables huggingface.prompt_builder = \"llama2\" huggingface.api_base = \"YOUR_ENDPOINT_URL\"  # The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file. # huggingface.api_key=\"hf_xxx\"  response = huggingface.ChatCompletion.create(     messages=[         {\"role\": \"system\", \"content\": \"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"},         {\"role\": \"user\", \"content\": \"Knock knock.\"},         {\"role\": \"assistant\", \"content\": \"Who's there?\"},         {\"role\": \"user\", \"content\": \"Apple.\"},     ],       temperature=0.9,       top_p=0.6,       max_tokens=1024, ) response Out[1]: <pre>{'id': 'hf-0lL5H_yyRR',\n 'object': 'chat.completion',\n 'created': 1691096023,\n 'choices': [{'index': 0,\n   'message': {'role': 'assistant', 'content': ' Apple who?'},\n   'finish_reason': 'eos_token'}],\n 'usage': {'prompt_tokens': 149, 'completion_tokens': 5, 'total_tokens': 154}}</pre> <p>As you can see, the response object has a few fields:</p> <ul> <li><code>id</code>: the ID of the request</li> <li><code>object</code>: the type of object returned (e.g., <code>chat.completion</code>)</li> <li><code>created</code>: the timestamp of the request</li> <li><code>model</code>: the full name of the model used to generate the response</li> <li><code>usage</code>: the number of tokens used to generate the replies, counting prompt, completion, and total</li> <li><code>choices</code>: a list of completion objects (only one, unless you set <code>n</code> greater than 1)<ul> <li><code>message</code>: the message object generated by the model, with <code>role</code> and <code>content</code></li> <li><code>finish_reason</code>: the reason the model stopped generating text (either <code>stop</code>, or <code>length</code> if <code>max_tokens</code> limit was reached)</li> <li><code>index</code>: the index of the completion in the list of choices</li> </ul> </li> </ul> <p>Extract just the reply with:</p> In\u00a0[2]: Copied! <pre>print(response['choices'][0]['message']['content'])\n</pre> print(response['choices'][0]['message']['content']) <pre> Apple who?\n</pre> In\u00a0[6]: Copied! <pre>from easyllm.clients import huggingface\n\nhuggingface.prompt_builder = \"llama2\"\n\n# Here you can overwrite the url to your endpoint, can also be localhost:8000\nhuggingface.api_base = \"YOUR_ENDPOINT_URL\"\n\n# a ChatCompletion request\nresponse = huggingface.ChatCompletion.create(\n    messages=[\n        {'role': 'user', 'content': \"Count to 10.\"}\n    ],\n    stream=True  # this time, we set stream=True\n)\n\nfor chunk in response:\n    delta = chunk['choices'][0]['delta']\n    if \"content\" in delta:\n        print(delta[\"content\"],end=\"\")\n</pre> from easyllm.clients import huggingface  huggingface.prompt_builder = \"llama2\"  # Here you can overwrite the url to your endpoint, can also be localhost:8000 huggingface.api_base = \"YOUR_ENDPOINT_URL\"  # a ChatCompletion request response = huggingface.ChatCompletion.create(     messages=[         {'role': 'user', 'content': \"Count to 10.\"}     ],     stream=True  # this time, we set stream=True )  for chunk in response:     delta = chunk['choices'][0]['delta']     if \"content\" in delta:         print(delta[\"content\"],end=\"\") <pre>  Sure! Here we go:\n\n1. One\n2. Two\n3. Three\n4. Four\n5. Five\n6. Six\n7. Seven\n8. Eight\n9. Nine\n10. Ten!</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/inference-endpoints-example/#hugging-face-inference-endpoints-example","title":"Hugging Face Inference Endpoints Example\u00b6","text":"<p>Hugging Face Inference Endpoints\u00a0offers an easy and secure way to deploy Machine Learning models for use in production. Inference Endpoints empower developers and data scientists alike to create AI applications without managing infrastructure: simplifying the deployment process to a few clicks, including handling large volumes of requests with autoscaling, reducing infrastructure costs with scale-to-zero, and offering advanced security.</p> <p>You can get started with Inference Endpoints at:\u00a0https://ui.endpoints.huggingface.co/</p> <p>The example assumes that you have an running endpoint for a conversational model, e.g. <code>https://huggingface.co/meta-llama/Llama-2-13b-chat-hf</code></p>"},{"location":"examples/inference-endpoints-example/#1-import-the-easyllm-library","title":"1. Import the easyllm library\u00b6","text":""},{"location":"examples/inference-endpoints-example/#2-an-example-chat-api-call","title":"2. An example chat API call\u00b6","text":"<p>Since we want to use our endpoint for inference we don't have to define the <code>model</code> parameter. We either need to expose an environment variable <code>HUGGINGFACE_API_BASE</code> before the import of <code>easyllm.clients.huggingface</code> or overwrite the <code>huggingface.api_base</code> value.</p> <p>A chat API call then only has two required inputs:</p> <ul> <li><code>messages</code>: a list of message objects, where each object has two required fields:<ul> <li><code>role</code>: the role of the messenger (either <code>system</code>, <code>user</code>, or <code>assistant</code>)</li> <li><code>content</code>: the content of the message (e.g., <code>Write me a beautiful poem</code>)</li> </ul> </li> </ul> <p>Let's look at an example chat API calls to see how the chat format works in practice.</p>"},{"location":"examples/inference-endpoints-example/#how-to-stream-chat-completion-requests","title":"How to stream Chat Completion requests\u00b6","text":"<p>Custom endpoints can be created to stream chat completion requests to a model.</p>"},{"location":"examples/llama2-agent-example/","title":"Llama 2 70B Agent/Tool use example","text":"In\u00a0[\u00a0]: Copied! <pre># if needed, install and/or upgrade to the latest version of the EasyLLM Python library\n%pip install --upgrade easyllm\n</pre> # if needed, install and/or upgrade to the latest version of the EasyLLM Python library %pip install --upgrade easyllm  <p>Getting an open LLM to act like an agent or use tools is incredibly hard. However, with Llama 2 70B it is now possible. Let's see how we can get it running!</p> In\u00a0[41]: Copied! <pre>system_message = \"\"\"Assistant is a expert JSON builder designed to assist with a wide range of tasks.\n\nAssistant is able to respond to the User and use tools using JSON strings that contain \"action\" and \"action_input\" parameters.\n\nAll of Assistant's communication is performed using this JSON format.\n\nAssistant can also use tools by responding to the user with tool use instructions in the same \"action\" and \"action_input\" JSON format. Tools available to Assistant are:\n\n- \"Calculator\": Useful for when you need to answer questions about math.\n  - To use the calculator tool, Assistant should write like so:\n    ```json\n    {{\"action\": \"Calculator\",\n      \"action_input\": \"4+4\"}}\n    ```\n\nHere are some previous conversations between the Assistant and User:\n\nUser: Hey how are you today?\nAssistant: ```json\n{{\"action\": \"Final Answer\",\n \"action_input\": \"I'm good thanks, how are you?\"}}\n```\nUser: I'm great, what is the square root of 4?\nAssistant: ```json\n{{\"action\": \"Calculator\",\n \"action_input\": \"sqrt(4)\"}}\n```\nResult: 2.0\nAssistant: ```json\n{{\"action\": \"Final Answer\",\n \"action_input\": \"It looks like the answer is 2!\"}}\n```\nUser: Thanks could you tell me what 4 to the power of 2 is?\nAssistant: ```json\n{{\"action\": \"Calculator\",\n \"action_input\": \"4**2\"}}\n```\nResult: 16.0\nAssistant: ```json\n{{\"action\": \"Final Answer\",\n \"action_input\": \"It looks like the answer is 16!\"}}\n```\n\nHere is the latest conversation between Assistant and User.\"\"\"\n</pre> system_message = \"\"\"Assistant is a expert JSON builder designed to assist with a wide range of tasks.  Assistant is able to respond to the User and use tools using JSON strings that contain \"action\" and \"action_input\" parameters.  All of Assistant's communication is performed using this JSON format.  Assistant can also use tools by responding to the user with tool use instructions in the same \"action\" and \"action_input\" JSON format. Tools available to Assistant are:  - \"Calculator\": Useful for when you need to answer questions about math.   - To use the calculator tool, Assistant should write like so:     ```json     {{\"action\": \"Calculator\",       \"action_input\": \"4+4\"}}     ```  Here are some previous conversations between the Assistant and User:  User: Hey how are you today? Assistant: ```json {{\"action\": \"Final Answer\",  \"action_input\": \"I'm good thanks, how are you?\"}} ``` User: I'm great, what is the square root of 4? Assistant: ```json {{\"action\": \"Calculator\",  \"action_input\": \"sqrt(4)\"}} ``` Result: 2.0 Assistant: ```json {{\"action\": \"Final Answer\",  \"action_input\": \"It looks like the answer is 2!\"}} ``` User: Thanks could you tell me what 4 to the power of 2 is? Assistant: ```json {{\"action\": \"Calculator\",  \"action_input\": \"4**2\"}} ``` Result: 16.0 Assistant: ```json {{\"action\": \"Final Answer\",  \"action_input\": \"It looks like the answer is 16!\"}} ```  Here is the latest conversation between Assistant and User.\"\"\" <p>In addition to our system message which holds the information for our tools we need to create a user template, which includes the input from the user and tells the model to use tools or not.</p> In\u00a0[42]: Copied! <pre>prompt = f\"{system_message}\\n\\nUse your existing tools and respond with a JSON object with with 'action' and 'action_input' values \\nUser: {{user_input}}\"\n</pre> prompt = f\"{system_message}\\n\\nUse your existing tools and respond with a JSON object with with 'action' and 'action_input' values \\nUser: {{user_input}}\" <p>Now lets combine both and create a request using <code>easyllm</code>.</p> In\u00a0[43]: Copied! <pre>from easyllm.clients import huggingface\n\n# Changing configuration without using environment variables\nhuggingface.prompt_builder = \"llama2\"\n# huggingface.api_key=\"hf_xxx\"\n\ndef agent(prompt):\n  response = huggingface.Completion.create(\n      model=\"meta-llama/Llama-2-70b-chat-hf\",\n      prompt=prompt,\n      temperature=0.1,\n      max_tokens=128,\n      stop=[\"```\\n\",\"Result: \"],\n      debug=False,\n  )  \n  return response[\"choices\"][0][\"text\"]\n</pre>  from easyllm.clients import huggingface  # Changing configuration without using environment variables huggingface.prompt_builder = \"llama2\" # huggingface.api_key=\"hf_xxx\"  def agent(prompt):   response = huggingface.Completion.create(       model=\"meta-llama/Llama-2-70b-chat-hf\",       prompt=prompt,       temperature=0.1,       max_tokens=128,       stop=[\"```\\n\",\"Result: \"],       debug=False,   )     return response[\"choices\"][0][\"text\"] <p>Now we can begin asking questions</p> In\u00a0[44]: Copied! <pre>output = agent(prompt.format(user_input=\"hey how are you today?\"))\noutput\n</pre> output = agent(prompt.format(user_input=\"hey how are you today?\")) output Out[44]: <pre>' Assistant: ```json\\n{\"action\": \"Final Answer\",\\n \"action_input\": \"I\\'m good thanks, how are you?\"}\\n```'</pre> <p>What happens if we ask a math question?</p> In\u00a0[45]: Copied! <pre>output = agent(prompt.format(user_input=\"What is 4 multiplied by 2?\"))\noutput\n</pre> output = agent(prompt.format(user_input=\"What is 4 multiplied by 2?\")) output Out[45]: <pre>' Assistant: ```json\\n{\"action\": \"Calculator\",\\n \"action_input\": \"4*2\"}\\n```\\n'</pre> <p>Great! It works! It correctly selects the tool. Okay now to make it work we need to parse the output and execute it in the case for the calculator</p> In\u00a0[46]: Copied! <pre>import json\nimport re\n\ndef parser(input):\n    pattern = r'```json\\n(.*?)```'\n    match = re.search(pattern, input, re.DOTALL)\n    if not match:\n        raise ValueError(\"Couldn't parse the output.\")\n    \n    parsed_data = json.loads(match.group(1))\n    return parsed_data\n</pre> import json import re  def parser(input):     pattern = r'```json\\n(.*?)```'     match = re.search(pattern, input, re.DOTALL)     if not match:         raise ValueError(\"Couldn't parse the output.\")          parsed_data = json.loads(match.group(1))     return parsed_data       In\u00a0[47]: Copied! <pre>output = parser(output)\noutput\n</pre> output = parser(output) output Out[47]: <pre>{'action': 'Calculator', 'action_input': '4*2'}</pre> <p>Okay, Now lets execute it using the <code>eval</code> function from python</p> In\u00a0[48]: Copied! <pre>def use_tool(tool,tool_input):\n  if tool == \"Calculator\":\n    return eval(tool_input)\n  else:\n    raise Exception(\"Unknown tool: \" + tool)\n</pre> def use_tool(tool,tool_input):   if tool == \"Calculator\":     return eval(tool_input)   else:     raise Exception(\"Unknown tool: \" + tool) <p>Okay, now lets combine everyting and the cacluator result to our agent again.</p> In\u00a0[73]: Copied! <pre>def use_calculator(input, first_call=True):\n  if first_call:\n    input_prompt = prompt.format(user_input=input)\n  else:\n    input_prompt = input\n  # make the agent call\n  response = agent(input_prompt)\n  # parse the output if possible \n  parsed = parser(response)\n  # check if the output is our final answer or if it is a tool\n  if parsed[\"action\"] == \"Final Answer\":\n    return parsed[\"action_input\"]\n  # if not try to use the tool\n  tool_output = use_tool(parsed[\"action\"], parsed[\"action_input\"])\n  \n  # add message to the agent\n  next_prompt = f\"{input_prompt}\\n{response}\\nResponse: {tool_output}\"\n  # recursively call the agent with the output of the tool\n  return use_calculator(next_prompt, False)\n</pre> def use_calculator(input, first_call=True):   if first_call:     input_prompt = prompt.format(user_input=input)   else:     input_prompt = input   # make the agent call   response = agent(input_prompt)   # parse the output if possible    parsed = parser(response)   # check if the output is our final answer or if it is a tool   if parsed[\"action\"] == \"Final Answer\":     return parsed[\"action_input\"]   # if not try to use the tool   tool_output = use_tool(parsed[\"action\"], parsed[\"action_input\"])      # add message to the agent   next_prompt = f\"{input_prompt}\\n{response}\\nResponse: {tool_output}\"   # recursively call the agent with the output of the tool   return use_calculator(next_prompt, False)       In\u00a0[75]: Copied! <pre>use_calculator(\"What is 19 * 11?\")\n</pre> use_calculator(\"What is 19 * 11?\") Out[75]: <pre>'It looks like the answer is 209!'</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/llama2-agent-example/#llama-2-70b-agenttool-use-example","title":"Llama 2 70B Agent/Tool use example\u00b6","text":"<p>This Jupyter notebook provides examples of how to use Tools for Agents with the Llama 2 70B model in EasyLLM. This includes an example on how to use tools with an LLM, including output parsing, execution of the tools and parsing of the results. It is a very simplified example. If you are interested in Agents you should checkout langchain or the ReAct pattern.</p>"},{"location":"examples/llama2-agent-example/#why-do-llms-need-to-use-tools","title":"Why do LLMs need to use Tools?\u00b6","text":"<p>One of the most common challenges with LLMs is overcoming the lack of recency and specificity in their training data - answers can be out of date, and they are prone to hallucinations given the huge variety in their knowledge base. Tools are a great method of allowing an LLM to answer within a controlled context that draws on your existing knowledge bases and internal APIs - instead of trying to prompt engineer the LLM all the way to your intended answer, you allow it access to tools that it calls on dynamically for info, parses, and serves to customer.</p> <p>Providing LLMs access to tools can enable them to answer questions with context directly from search engines, APIs or your own databases. Instead of answering directly, an LLM with access to tools can perform intermediate steps to gather relevant information. Tools can also be used in combination. For example, a language model can be made to use a search tool to lookup quantitative information and a calculator to execute calculations.</p>"},{"location":"examples/llama2-agent-example/#basic-example-of-using-a-tool-with-llama-2-70b","title":"Basic example of using a tool with Llama 2 70B\u00b6","text":"<p>In the basic we are are going to only use one abstract tool, a <code>calculator</code>. Our model can use the calculator run mathmatical operations. To make it easy we provide some few-shot example for the model to better understand what it needs to do. Note: This is adapted from the example by pinecone.</p>"},{"location":"examples/llama2-rag-example/","title":"Retrieval Augmented Generation using Llama 2","text":"In\u00a0[\u00a0]: Copied! <pre># if needed, install and/or upgrade to the latest version of the EasyLLM Python library\n%pip install --upgrade easyllm\n</pre> # if needed, install and/or upgrade to the latest version of the EasyLLM Python library %pip install --upgrade easyllm  In\u00a0[8]: Copied! <pre>SYSTEM_PROMPT = \"\"\"You are an AI assistant that answers questions in a friendly manner, based on the given #SOURCE# documents. Here are some rules you always follow:\n- Generate human readable output, avoid creating output with gibberish text.\n- Generate only the requested output, don't include any other language before or after the requested output.\n- Never say thank you, that you are happy to help, that you are an AI agent, etc. Just answer directly.\n- Generate professional language typically used in business documents in North America.\n- Never generate offensive or foul language.\n- Only include facts and information based on the #SOURCE# documents.\n\"\"\"\n\nsystem = {\"role\": \"system\", \"content\": SYSTEM_PROMPT}\n</pre> SYSTEM_PROMPT = \"\"\"You are an AI assistant that answers questions in a friendly manner, based on the given #SOURCE# documents. Here are some rules you always follow: - Generate human readable output, avoid creating output with gibberish text. - Generate only the requested output, don't include any other language before or after the requested output. - Never say thank you, that you are happy to help, that you are an AI agent, etc. Just answer directly. - Generate professional language typically used in business documents in North America. - Never generate offensive or foul language. - Only include facts and information based on the #SOURCE# documents. \"\"\"  system = {\"role\": \"system\", \"content\": SYSTEM_PROMPT} <p>before we can now call our LLM. Lets create a user instruction with a <code>query</code> and a <code>context</code>. As a context i copied the the wikipedia article of Nuremberg (the city i live). I uploaded it as a gist to to not pollute the notebook.</p> In\u00a0[\u00a0]: Copied! <pre>!wget https://gist.githubusercontent.com/philschmid/2678351cb9f41d385aa5c099caf20c0a/raw/60ae425677dd9bed6fe3c0f2dd5b6ea49bc6590c/nuremberg.txt\n</pre> !wget https://gist.githubusercontent.com/philschmid/2678351cb9f41d385aa5c099caf20c0a/raw/60ae425677dd9bed6fe3c0f2dd5b6ea49bc6590c/nuremberg.txt In\u00a0[14]: Copied! <pre>context = open(\"nuremberg.txt\").read()\n\nquery = \"How many people live in Nuremberg?\"\n</pre> context = open(\"nuremberg.txt\").read()  query = \"How many people live in Nuremberg?\" <p>Before we use our context lets just ask the model.</p> In\u00a0[15]: Copied! <pre>from easyllm.clients import huggingface\n\n# set the prompt builder to llama2\nhuggingface.prompt_builder = \"llama2\"\n# huggingface.api_key = \"hf_xx\"\n\n# send a ChatCompletion request\nresponse = huggingface.ChatCompletion.create(\n    model=\"meta-llama/Llama-2-70b-chat-hf\",\n    messages=[\n        {\"role\": \"user\", \"content\": query},\n    ],\n)\n\n# print the time delay and text received\nprint(response[\"choices\"][0][\"message\"][\"content\"])\n</pre> from easyllm.clients import huggingface  # set the prompt builder to llama2 huggingface.prompt_builder = \"llama2\" # huggingface.api_key = \"hf_xx\"  # send a ChatCompletion request response = huggingface.ChatCompletion.create(     model=\"meta-llama/Llama-2-70b-chat-hf\",     messages=[         {\"role\": \"user\", \"content\": query},     ], )  # print the time delay and text received print(response[\"choices\"][0][\"message\"][\"content\"])  <pre> As of December 31, 2020, the population of Nuremberg, Germany is approximately 516,000 people.\n</pre> <p>Now lets use our <code>system</code> message with our <code>context</code> to augment the knowledge of our model \"in-memory\" and ask the same question again.</p> In\u00a0[23]: Copied! <pre>context_extended = f\"{query}\\n\\n#SOURCE#\\n{context}\"\n# context_extended = f\"{query}\\n\\n#SOURCE START#\\n{context}\\n#SOURCE END#{query}\"\n</pre> context_extended = f\"{query}\\n\\n#SOURCE#\\n{context}\" # context_extended = f\"{query}\\n\\n#SOURCE START#\\n{context}\\n#SOURCE END#{query}\" In\u00a0[22]: Copied! <pre>from easyllm.clients import huggingface\n\n# set the prompt builder to llama2\nhuggingface.prompt_builder = \"llama2\"\n# huggingface.api_key = \"hf_xx\"\n\n# send a ChatCompletion request\nresponse = huggingface.ChatCompletion.create(\n    model=\"meta-llama/Llama-2-70b-chat-hf\",\n    messages=[\n        system, \n        {\"role\": \"user\", \"content\": context_extended},\n    ],\n)\n\n# print the time delay and text received\nprint(response[\"choices\"][0][\"message\"][\"content\"])\n</pre> from easyllm.clients import huggingface  # set the prompt builder to llama2 huggingface.prompt_builder = \"llama2\" # huggingface.api_key = \"hf_xx\"  # send a ChatCompletion request response = huggingface.ChatCompletion.create(     model=\"meta-llama/Llama-2-70b-chat-hf\",     messages=[         system,          {\"role\": \"user\", \"content\": context_extended},     ], )  # print the time delay and text received print(response[\"choices\"][0][\"message\"][\"content\"])  <pre> The population of Nuremberg is 523,026 according to the 2022-12-31 data.\n</pre> <p>Awesome! if we check the gist, we can see a snippet in there with saying</p> <pre>Population (2022-12-31)[2]\n\u2022 City\t523,026\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/llama2-rag-example/#retrieval-augmented-generation-using-llama-2","title":"Retrieval Augmented Generation using Llama 2\u00b6","text":"<p>This notebook walks through how to use Llama 2 to perform (in-context) retrieval augmented generation. We will customize the <code>system</code> message for Llama 2 to make sure the model is only using provided context to generate the response.</p> <p>What is In-context Retrieval Augmented Generation?</p> <p>In-context retrieval augmented generation is a method to improve language model generation by including relevant documents to the model input. The key points are:</p> <ul> <li>Retrieval of relevant documents from an external corpus to provide factual grounding for the model.</li> <li>Prepending the retrieved documents to the input text, without modifying the model architecture or fine-tuning the model.</li> <li>Allows leveraging external knowledge with off-the-shelf frozen language models.</li> </ul>"},{"location":"examples/llama2-rag-example/#simple-example","title":"Simple Example\u00b6","text":"<p>Below is a simple example using the existing prompt builder of llama2 to generate a prompt. We are going to use the <code>system</code> message from llama-index with some minor adjustments.</p>"},{"location":"examples/llama2-rag-example/#next-steps","title":"Next Steps\u00b6","text":"<p>Next steps, would be to connect your LLM using with external knowledge sources such as Wikis, the Web or other databases using tools and APIs or vector databases and embeddings.</p>"},{"location":"examples/stream-chat-completions/","title":"How to stream Chat Completion requests","text":"In\u00a0[1]: Copied! <pre># imports\nimport easyllm  # for API calls\nimport time  # for measuring time duration of API calls\n</pre> # imports import easyllm  # for API calls import time  # for measuring time duration of API calls In\u00a0[3]: Copied! <pre>from easyllm.clients import huggingface\n\n# set the prompt builder to llama2\nhuggingface.prompt_builder = \"llama2\"\n\n# record the time before the request is sent\nstart_time = time.time()\n\n# send a ChatCompletion request to count to 100\nresponse = huggingface.ChatCompletion.create(\n    model=\"meta-llama/Llama-2-70b-chat-hf\",\n    messages=[\n        {'role': 'user', 'content': 'Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}\n    ],\n)\n\n# calculate the time it took to receive the response\nresponse_time = time.time() - start_time\n\n# print the time delay and text received\nprint(f\"Full response received {response_time:.2f} seconds after request\")\nprint(f\"Full response received:\\n{response}\")\n</pre> from easyllm.clients import huggingface  # set the prompt builder to llama2 huggingface.prompt_builder = \"llama2\"  # record the time before the request is sent start_time = time.time()  # send a ChatCompletion request to count to 100 response = huggingface.ChatCompletion.create(     model=\"meta-llama/Llama-2-70b-chat-hf\",     messages=[         {'role': 'user', 'content': 'Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}     ], )  # calculate the time it took to receive the response response_time = time.time() - start_time  # print the time delay and text received print(f\"Full response received {response_time:.2f} seconds after request\") print(f\"Full response received:\\n{response}\")  <pre>Full response received 0.12 seconds after request\nFull response received:\n{'id': 'hf-JhxbFCGVUW', 'object': 'chat.completion', 'created': 1691129826, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': ' Sure! Here it is:\\n\\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100'}, 'finish_reason': 'eos_token'}], 'usage': {'prompt_tokens': 25, 'completion_tokens': 400, 'total_tokens': 425}}\n</pre> <p>The reply can be extracted with <code>response['choices'][0]['message']</code>.</p> <p>The content of the reply can be extracted with <code>response['choices'][0]['message']['content']</code>.</p> In\u00a0[4]: Copied! <pre>reply = response['choices'][0]['message']\nprint(f\"Extracted reply: \\n{reply}\")\n\nreply_content = response['choices'][0]['message']['content']\nprint(f\"Extracted content: \\n{reply_content}\")\n</pre> reply = response['choices'][0]['message'] print(f\"Extracted reply: \\n{reply}\")  reply_content = response['choices'][0]['message']['content'] print(f\"Extracted content: \\n{reply_content}\")  <pre>Extracted reply: \n{'role': 'assistant', 'content': ' Sure! Here it is:\\n\\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100'}\nExtracted content: \n Sure! Here it is:\n\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100\n</pre> In\u00a0[5]: Copied! <pre>from easyllm.clients import huggingface\n\n\nhuggingface.prompt_builder = \"llama2\"\n\n# a ChatCompletion request\nresponse = huggingface.ChatCompletion.create(\n    model=\"meta-llama/Llama-2-70b-chat-hf\",\n    messages=[\n        {'role': 'user', 'content': \"What's 1+1? Answer in one word.\"}\n    ],\n    stream=True  # this time, we set stream=True\n)\n\nfor chunk in response:\n    print(chunk)\n</pre> from easyllm.clients import huggingface   huggingface.prompt_builder = \"llama2\"  # a ChatCompletion request response = huggingface.ChatCompletion.create(     model=\"meta-llama/Llama-2-70b-chat-hf\",     messages=[         {'role': 'user', 'content': \"What's 1+1? Answer in one word.\"}     ],     stream=True  # this time, we set stream=True )  for chunk in response:     print(chunk) <pre>{'id': 'hf--G5Fhg3YMu', 'object': 'chat.completion.chunk', 'created': 1691129830, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': 'assistant'}}]}\n{'id': 'hf--G5Fhg3YMu', 'object': 'chat.completion.chunk', 'created': 1691129830, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'content': ' '}}]}\n{'id': 'hf--G5Fhg3YMu', 'object': 'chat.completion.chunk', 'created': 1691129830, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'content': ' Two'}}]}\n{'id': 'hf--G5Fhg3YMu', 'object': 'chat.completion.chunk', 'created': 1691129830, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {}}]}\n</pre> <p>As you can see above, streaming responses have a <code>delta</code> field rather than a <code>message</code> field. <code>delta</code> can hold things like:</p> <ul> <li>a role token (e.g., <code>{\"role\": \"assistant\"}</code>)</li> <li>a content token (e.g., <code>{\"content\": \"\\n\\n\"}</code>)</li> <li>nothing (e.g., <code>{}</code>), when the stream is over</li> </ul> In\u00a0[6]: Copied! <pre>import time\nfrom easyllm.clients import huggingface\n\nhuggingface.prompt_builder = \"llama2\"\n\n# record the time before the request is sent\nstart_time = time.time()\n\n# send a ChatCompletion request to count to 100\nresponse = huggingface.ChatCompletion.create(\n    model=\"meta-llama/Llama-2-70b-chat-hf\",\n    messages=[\n        {'role': 'user', 'content': 'Count to 50, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}\n    ],\n    stream=True  # again, we set stream=True\n)\n\n# create variables to collect the stream of chunks\ncollected_chunks = []\ncollected_messages = []\n# iterate through the stream of events\nfor chunk in response:\n    chunk_time = time.time() - start_time  # calculate the time delay of the chunk\n    collected_chunks.append(chunk)  # save the event response\n    chunk_message = chunk['choices'][0]['delta']  # extract the message\n    collected_messages.append(chunk_message)  # save the message\n    print(f\"Message received {chunk_time:.2f} seconds after request: {chunk_message}\")  # print the delay and text\n\n# print the time delay and text received\nprint(f\"Full response received {chunk_time:.2f} seconds after request\")\nfull_reply_content = ''.join([m.get('content', '') for m in collected_messages])\nprint(f\"Full conversation received: {full_reply_content}\")\n</pre> import time from easyllm.clients import huggingface  huggingface.prompt_builder = \"llama2\"  # record the time before the request is sent start_time = time.time()  # send a ChatCompletion request to count to 100 response = huggingface.ChatCompletion.create(     model=\"meta-llama/Llama-2-70b-chat-hf\",     messages=[         {'role': 'user', 'content': 'Count to 50, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}     ],     stream=True  # again, we set stream=True )  # create variables to collect the stream of chunks collected_chunks = [] collected_messages = [] # iterate through the stream of events for chunk in response:     chunk_time = time.time() - start_time  # calculate the time delay of the chunk     collected_chunks.append(chunk)  # save the event response     chunk_message = chunk['choices'][0]['delta']  # extract the message     collected_messages.append(chunk_message)  # save the message     print(f\"Message received {chunk_time:.2f} seconds after request: {chunk_message}\")  # print the delay and text  # print the time delay and text received print(f\"Full response received {chunk_time:.2f} seconds after request\") full_reply_content = ''.join([m.get('content', '') for m in collected_messages]) print(f\"Full conversation received: {full_reply_content}\")  <pre>Message received 0.13 seconds after request: {'role': 'assistant'}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': ' Sure'}\nMessage received 0.13 seconds after request: {'content': '!'}\nMessage received 0.13 seconds after request: {'content': ' Here'}\nMessage received 0.13 seconds after request: {'content': ' it'}\nMessage received 0.13 seconds after request: {'content': ' is'}\nMessage received 0.13 seconds after request: {'content': ':'}\nMessage received 0.13 seconds after request: {'content': '\\n'}\nMessage received 0.13 seconds after request: {'content': '\\n'}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '5'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '6'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '7'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '8'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '9'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': '0'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': '5'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': '6'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': '7'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': '8'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': '9'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': '0'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': '5'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': '6'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': '7'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': '8'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': '9'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': '0'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': '5'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': '6'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': '7'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': '8'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': '9'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': '0'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': '1'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': '2'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': '3'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': '5'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': '6'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': '7'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': '8'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '4'}\nMessage received 0.13 seconds after request: {'content': '9'}\nMessage received 0.13 seconds after request: {'content': ','}\nMessage received 0.13 seconds after request: {'content': ' '}\nMessage received 0.13 seconds after request: {'content': '5'}\nMessage received 0.13 seconds after request: {'content': '0'}\nMessage received 0.13 seconds after request: {}\nFull response received 0.13 seconds after request\nFull conversation received:   Sure! Here it is:\n\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50\n</pre>"},{"location":"examples/stream-chat-completions/#how-to-stream-chat-completion-requests","title":"How to stream Chat Completion requests\u00b6","text":"<p>By default, when you request a completion, the entire completion is generated before being sent back in a single response.</p> <p>If you're generating long completions, waiting for the response can take many seconds.</p> <p>To get responses sooner, you can 'stream' the completion as it's being generated. This allows you to start printing or processing the beginning of the completion before the full completion is finished.</p> <p>To stream completions, set <code>stream=True</code> when calling the chat completions or completions endpoints. This will return an object that streams back the response as data-only server-sent events. Extract chunks from the <code>delta</code> field rather than the <code>message</code> field.</p>"},{"location":"examples/stream-chat-completions/#downsides","title":"Downsides\u00b6","text":"<p>Note that using <code>stream=True</code> in a production application makes it more difficult to moderate the content of the completions, as partial completions may be more difficult to evaluate.</p>"},{"location":"examples/stream-chat-completions/#example-code","title":"Example code\u00b6","text":"<p>Below, this notebook shows:</p> <ol> <li>What a typical chat completion response looks like</li> <li>What a streaming chat completion response looks like</li> <li>How much time is saved by streaming a chat completion</li> </ol>"},{"location":"examples/stream-chat-completions/#1-what-a-typical-chat-completion-response-looks-like","title":"1. What a typical chat completion response looks like\u00b6","text":"<p>With a typical ChatCompletions API call, the response is first computed and then returned all at once.</p>"},{"location":"examples/stream-chat-completions/#2-how-to-stream-a-chat-completion","title":"2. How to stream a chat completion\u00b6","text":"<p>With a streaming API call, the response is sent back incrementally in chunks via an event stream. In Python, you can iterate over these events with a <code>for</code> loop.</p> <p>Let's see what it looks like:</p>"},{"location":"examples/stream-chat-completions/#3-how-much-time-is-saved-by-streaming-a-chat-completion","title":"3. How much time is saved by streaming a chat completion\u00b6","text":"<p>Now let's ask <code>meta-llama/Llama-2-70b-chat-hf</code> to count to 100 again, and see how long it takes.</p>"},{"location":"examples/stream-chat-completions/#time-comparison","title":"Time comparison\u00b6","text":"<p>In the example above, both requests took about 3 seconds to fully complete. Request times will vary depending on load and other stochastic factors.</p> <p>However, with the streaming request, we received the first token after 0.1 seconds, and subsequent tokens every ~0.01-0.02 seconds.</p>"},{"location":"examples/stream-text-completions/","title":"How to stream Text Completion requests","text":"In\u00a0[1]: Copied! <pre># imports\nimport easyllm  # for API calls\nimport time  # for measuring time duration of API calls\n</pre> # imports import easyllm  # for API calls import time  # for measuring time duration of API calls In\u00a0[1]: Copied! <pre>from easyllm.clients import huggingface\n\nhuggingface.prompt_builder = \"llama2\"\n\n# a ChatCompletion request\nresponse = huggingface.Completion.create(\n    model=\"meta-llama/Llama-2-70b-chat-hf\",\n    prompt=\"Count to 50, with a comma between each number and no newlines. E.g., 1, 2, 3, ...\",\n    stream=True  # this time, we set stream=True\n)\n\nfor chunk in response:\n    print(chunk)\n</pre> from easyllm.clients import huggingface  huggingface.prompt_builder = \"llama2\"  # a ChatCompletion request response = huggingface.Completion.create(     model=\"meta-llama/Llama-2-70b-chat-hf\",     prompt=\"Count to 50, with a comma between each number and no newlines. E.g., 1, 2, 3, ...\",     stream=True  # this time, we set stream=True )  for chunk in response:     print(chunk) <pre>{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' Sure', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '!', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' Here', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' it', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' is', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ':', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '\\n', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '\\n', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '5', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '6', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '7', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '8', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '9', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '0', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '5', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '6', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '7', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '8', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '9', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '0', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '5', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '6', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '7', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '8', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '9', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '0', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '5', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '6', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '7', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '8', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '9', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '0', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '5', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '6', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '7', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '8', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '9', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '5', 'logprobs': 0.0}]}\n{'id': 'hf-PokypeK2an', 'object': 'text.completion', 'created': 1691129904, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '0', 'logprobs': 0.0}]}\n</pre> <p>As you can see above, streaming responses have a <code>text</code> field which holds the generated tokens.</p> <p>Below is an example where we print the generated text as it comes in, and stop when we see a <code>&lt;/s&gt;</code> token.</p> In\u00a0[2]: Copied! <pre>from easyllm.clients import huggingface\n\nhuggingface.prompt_builder = \"llama2\"\n\n# a ChatCompletion request\nresponse = huggingface.Completion.create(\n    model=\"meta-llama/Llama-2-70b-chat-hf\",\n    prompt=\"Count to 10, with a comma between each number and no newlines. E.g., 1, 2, 3, ...\",\n    stream=True  # this time, we set stream=True\n)\n\nfor chunk in response:\n    print(chunk[\"choices\"][0][\"text\"],end=\"\")\n</pre> from easyllm.clients import huggingface  huggingface.prompt_builder = \"llama2\"  # a ChatCompletion request response = huggingface.Completion.create(     model=\"meta-llama/Llama-2-70b-chat-hf\",     prompt=\"Count to 10, with a comma between each number and no newlines. E.g., 1, 2, 3, ...\",     stream=True  # this time, we set stream=True )  for chunk in response:     print(chunk[\"choices\"][0][\"text\"],end=\"\") <pre>  Sure! Here it is: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10.</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/stream-text-completions/#how-to-stream-text-completion-requests","title":"How to stream Text Completion requests\u00b6","text":"<p>By default, when you request a completion, the entire completion is generated before being sent back in a single response.</p> <p>If you're generating long completions, waiting for the response can take many seconds.</p> <p>To get responses sooner, you can 'stream' the completion as it's being generated. This allows you to start printing or processing the beginning of the completion before the full completion is finished.</p> <p>To stream completions, set <code>stream=True</code> when calling the chat completions or completions endpoints. This will return an object that streams back the response as data-only server-sent events. Extract chunks from the <code>delta</code> field rather than the <code>message</code> field.</p>"},{"location":"examples/stream-text-completions/#downsides","title":"Downsides\u00b6","text":"<p>Note that using <code>stream=True</code> in a production application makes it more difficult to moderate the content of the completions, as partial completions may be more difficult to evaluate.</p>"},{"location":"examples/stream-text-completions/#example-code","title":"Example code\u00b6","text":"<p>Below, this notebook shows:</p> <ol> <li>What a streaming text completion response looks like</li> </ol>"},{"location":"examples/stream-text-completions/#2-how-to-stream-a-text-completion","title":"2. How to stream a text completion\u00b6","text":"<p>With a streaming API call, the response is sent back incrementally in chunks via an event stream. In Python, you can iterate over these events with a <code>for</code> loop.</p> <p>Let's see what it looks like:</p>"},{"location":"examples/text-completion-api/","title":"# How to use Text (Instruction) Completion clients","text":"In\u00a0[\u00a0]: Copied! <pre># if needed, install and/or upgrade to the latest version of the OpenAI Python library\n%pip install --upgrade easyllm\n</pre> # if needed, install and/or upgrade to the latest version of the OpenAI Python library %pip install --upgrade easyllm  In\u00a0[6]: Copied! <pre># import the EasyLLM Python library for calling the EasyLLM API\nimport easyllm\n</pre> # import the EasyLLM Python library for calling the EasyLLM API import easyllm In\u00a0[1]: Copied! <pre>from easyllm.clients import huggingface\n\n# Example EasyLLM Python library request\nMODEL = \"meta-llama/Llama-2-70b-chat-hf\"\nhuggingface.prompt_builder = \"llama2\"\n\n# The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file.\n# huggingface.api_key=\"hf_xxx\"\n\nresponse = huggingface.Completion.create(\n    model=MODEL,\n    prompt=\"What is the meaning of life?\",\n    temperature=0.9,\n    top_p=0.6,\n    max_tokens=1024,\n)\nresponse\n</pre>  from easyllm.clients import huggingface  # Example EasyLLM Python library request MODEL = \"meta-llama/Llama-2-70b-chat-hf\" huggingface.prompt_builder = \"llama2\"  # The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file. # huggingface.api_key=\"hf_xxx\"  response = huggingface.Completion.create(     model=MODEL,     prompt=\"What is the meaning of life?\",     temperature=0.9,     top_p=0.6,     max_tokens=1024, ) response Out[1]: <pre>{'id': 'hf-ZK--Ndk30h',\n 'object': 'text.completion',\n 'created': 1691129933,\n 'model': 'meta-llama/Llama-2-70b-chat-hf',\n 'choices': [{'index': 0,\n   'text': \" The meaning of life is a question that has puzzled philosophers, theologians, and scientists for centuries. There are many different perspectives on what constitutes the meaning of life, and there is no one definitive answer. However, some common themes that people often associate with the meaning of life include:\\n\\n1. Purpose: Having a sense of purpose or direction in life, whether it be through work, relationships, or personal goals.\\n2. Fulfillment: Feeling fulfilled and satisfied with one's experiences and achievements.\\n3. Happiness: Pursuing happiness and well-being, whether through personal relationships, material possessions, or personal growth.\\n4. Self-actualization: Realizing one's potential and living up to one's capabilities.\\n5. Legacy: Leaving a lasting impact or legacy, whether through contributions to society, artistic or cultural achievements, or impacting the lives of others.\\n6. Spirituality: Connecting with a higher power or a sense of something greater than oneself, and finding meaning and purpose through faith or spiritual practices.\\n7. Love: Finding and experiencing love, whether it be through romantic relationships, friendships, or family.\\n8. Personal growth: Continuously learning, growing, and improving oneself.\\n9. Community: Building and being a part of a community, whether it be through work, volunteering, or social connections.\\n10. Making a difference: Making a positive impact in the world and leaving it a better place than when you arrived.\\n\\nUltimately, the meaning of life is a deeply personal and subjective question, and what gives meaning and purpose to one person's life may be different for another. It's a question that each person must answer for themselves, and it may change throughout their life as they grow and evolve.\",\n   'finish_reason': 'eos_token'}],\n 'usage': {'prompt_tokens': 11, 'completion_tokens': 406, 'total_tokens': 417}}</pre> <p>As you can see, the response object has a few fields:</p> <ul> <li><code>id</code>: the ID of the request</li> <li><code>object</code>: the type of object returned (e.g., <code>text.completion</code>)</li> <li><code>created</code>: the timestamp of the request</li> <li><code>model</code>: the full name of the model used to generate the response</li> <li><code>usage</code>: the number of tokens used to generate the replies, counting prompt, completion, and total</li> <li><code>choices</code>: a list of completion objects (only one, unless you set <code>n</code> greater than 1)<ul> <li><code>text</code>: the generated text</li> <li><code>finish_reason</code>: the reason the model stopped generating text (either <code>stop</code>, <code>eos_token</code>, or <code>length</code> if <code>max_tokens</code> limit was reached)</li> <li><code>logprobs</code>: optional the log probs of each generated token.</li> </ul> </li> </ul> <p>Extract just the reply with:</p> In\u00a0[2]: Copied! <pre>print(response['choices'][0]['text'])\n</pre> print(response['choices'][0]['text']) <pre> The meaning of life is a question that has puzzled philosophers, theologians, and scientists for centuries. There are many different perspectives on what constitutes the meaning of life, and there is no one definitive answer. However, some common themes that people often associate with the meaning of life include:\n\n1. Purpose: Having a sense of purpose or direction in life, whether it be through work, relationships, or personal goals.\n2. Fulfillment: Feeling fulfilled and satisfied with one's experiences and achievements.\n3. Happiness: Pursuing happiness and well-being, whether through personal relationships, material possessions, or personal growth.\n4. Self-actualization: Realizing one's potential and living up to one's capabilities.\n5. Legacy: Leaving a lasting impact or legacy, whether through contributions to society, artistic or cultural achievements, or impacting the lives of others.\n6. Spirituality: Connecting with a higher power or a sense of something greater than oneself, and finding meaning and purpose through faith or spiritual practices.\n7. Love: Finding and experiencing love, whether it be through romantic relationships, friendships, or family.\n8. Personal growth: Continuously learning, growing, and improving oneself.\n9. Community: Building and being a part of a community, whether it be through work, volunteering, or social connections.\n10. Making a difference: Making a positive impact in the world and leaving it a better place than when you arrived.\n\nUltimately, the meaning of life is a deeply personal and subjective question, and what gives meaning and purpose to one person's life may be different for another. It's a question that each person must answer for themselves, and it may change throughout their life as they grow and evolve.\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/text-completion-api/#how-to-use-text-instruction-completion-clients","title":"# How to use Text (Instruction) Completion clients\u00b6","text":"<p>EasyLLM can be used as an abstract layer to replace <code>text-davinci-003</code> with open source models.</p> <p>You can change your own applications from the OpenAI API, by simply changing the client.</p> <p>Chat models take a series of messages as input, and return an AI-written message as output.</p> <p>This guide illustrates the chat format with a few example API calls.</p>"},{"location":"examples/text-completion-api/#1-import-the-easyllm-library","title":"1. Import the easyllm library\u00b6","text":""},{"location":"examples/text-completion-api/#2-an-example-chat-api-call","title":"2. An example chat API call\u00b6","text":"<p>A text API call has two required inputs:</p> <ul> <li><code>model</code>: the name of the model you want to use (e.g., <code>meta-llama/Llama-2-70b-chat-hf</code>) or leave it empty to just call the api</li> <li><code>prompt</code>: a text prompt that is sent to the model to generate the text</li> </ul> <p>Compared to OpenAI api is the <code>huggingface</code> module also exposing a <code>prompt_builder</code> and <code>stop_sequences</code> parameter you can use to customize the prompt and stop sequences. The EasyLLM package comes with build in popular methods for both of these parameters, e.g. <code>llama2_prompt_builder</code> and <code>llama2_stop_sequences</code>.</p> <p>Let's look at an example chat API calls to see how the chat format works in practice.</p>"}]}